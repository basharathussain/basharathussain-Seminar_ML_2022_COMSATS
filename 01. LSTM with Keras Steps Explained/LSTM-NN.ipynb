{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LSTM-NN.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"mrPJnUokieIq"},"source":["https://colab.research.google.com/drive/1xN2COoN8HJhy7ILmuI47zekw2EPNlIAm#scrollTo=kgSuc3rYKo6y\n","https://github.com/keras-team/keras/tree/master/examples\n","\n","Connect your google drive \n"]},{"cell_type":"code","metadata":{"id":"ArOTVjJThTMc","outputId":"4ef7fe98-6746-4bbe-dff4-4be4fb13ae78","executionInfo":{"status":"ok","timestamp":1660973846427,"user_tz":-300,"elapsed":22938,"user":{"displayName":"Basharat Hussain","userId":"13397079541735527718"}},"colab":{"base_uri":"https://localhost:8080/"}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"XyYYuiI7itLz"},"source":["Load the dataset in data variable\n"]},{"cell_type":"code","metadata":{"id":"fUqjavZ3g8oW","executionInfo":{"status":"ok","timestamp":1660973850176,"user_tz":-300,"elapsed":2134,"user":{"displayName":"Basharat Hussain","userId":"13397079541735527718"}}},"source":["# here we will import the libraries used for machine learning\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv), data manipulation as in SQL\n","import matplotlib.pyplot as plt # this is used for the plot the graph \n","import seaborn as sns # used for plot interactive graph.\n","from sklearn.linear_model import LogisticRegression # to apply the Logistic regression\n","from sklearn.model_selection import train_test_split # to split the data into two parts\n","# from sklearn.cross_validation import KFold # use for cross validation\n","from sklearn.model_selection import GridSearchCV# for tuning parameter\n","from sklearn.ensemble import RandomForestClassifier # for random forest classifier\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn import svm # for Support Vector Machine\n","from sklearn import metrics # for the check the error and accuracy of the model\n","# Any results you write to the current directory are saved as output.\n","# dont worry about the error if its not working then insteda of model_selection we can use cross_validation\n"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"SC9n6yWYkqEY","outputId":"f581cc9b-bcaa-42e0-a3fa-a905a7c22298","executionInfo":{"status":"ok","timestamp":1660973858297,"user_tz":-300,"elapsed":1032,"user":{"displayName":"Basharat Hussain","userId":"13397079541735527718"}},"colab":{"base_uri":"https://localhost:8080/"}},"source":["# Get some time series data\n","data = pd.read_csv(\"/content/drive/My Drive/Google-CoLab/CourseML_2022_CAMSATS/11. Sample LSTM 001/timeseries-ds.csv\",\n","                     header=0)# here header 0 means the 0 th row is our coloumn \n","\n","# have a look at the data\n","print(data.head())# as u can see our data have imported and having 33 columns"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["         Date      A       B       C      D      E      F      G\n","0  2008-03-18  24.68  164.93  114.73  26.27  19.21  28.87  63.44\n","1  2008-03-19  24.18  164.89  114.75  26.22  19.07  27.76  59.98\n","2  2008-03-20  23.99  164.63  115.04  25.78  19.01  27.04  59.61\n","3  2008-03-25  24.14  163.92  114.85  27.41  19.61  27.84  59.41\n","4  2008-03-26  24.44  163.45  114.84  26.86  19.53  28.02  60.09\n"]}]},{"cell_type":"code","metadata":{"id":"7HUzL4QiKo6K","outputId":"989aaf78-7026-4c86-cb65-e1405be06270","executionInfo":{"status":"ok","timestamp":1660973860839,"user_tz":-300,"elapsed":414,"user":{"displayName":"Basharat Hussain","userId":"13397079541735527718"}},"colab":{"base_uri":"https://localhost:8080/"}},"source":["data.info()"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 11 entries, 0 to 10\n","Data columns (total 8 columns):\n"," #   Column  Non-Null Count  Dtype  \n","---  ------  --------------  -----  \n"," 0   Date    11 non-null     object \n"," 1   A       11 non-null     float64\n"," 2   B       11 non-null     float64\n"," 3   C       11 non-null     float64\n"," 4   D       11 non-null     float64\n"," 5   E       11 non-null     float64\n"," 6   F       11 non-null     float64\n"," 7   G       11 non-null     float64\n","dtypes: float64(7), object(1)\n","memory usage: 832.0+ bytes\n"]}]},{"cell_type":"code","metadata":{"id":"elQBfGhfmDAZ","outputId":"ee18bf50-fcbd-49c9-f772-9078e50fc7b2","executionInfo":{"status":"ok","timestamp":1572545905310,"user_tz":-300,"elapsed":987,"user":{"displayName":"Basharat Hussain","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDXtH5g0C7HMyOERBmIREHU1ZvtrNx98-DPcRb_=s64","userId":"13397079541735527718"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# now we can drop this column Unnamed: 32\n","##data.drop(\"samples_below_80pct_ff\",axis=0,inplace=True) # in this process this will change in our data itself \n","# if you want to save your old data then you can use below code\n","# data1=data.drop(\"Unnamed:32\",axis=1)\n","# here axis 1 means we are droping the column\n","\n","# data.drop(\"samples_below_75pct_ff\",axis=1,inplace=True)\n","# data.drop(\"samples_below_70pct_ff\",axis=1,inplace=True)\n","# data.drop(\"samples_below_65pct_ff\",axis=1,inplace=True)\n","# data.drop(\"samples_below_60pct_ff\",axis=1,inplace=True)\n","# data.drop(\"samples_below_55pct_ff\",axis=1,inplace=True)\n","# data.drop(\"samples_below_50pct_ff\",axis=1,inplace=True)\n","# data.drop(\"samples_below_45pct_ff\",axis=1,inplace=True)\n","# data.drop(\"samples_below_40pct_ff\",axis=1,inplace=True)\n","# data.drop(\"samples_below_35pct_ff\",axis=1,inplace=True)\n","# data.drop(\"samples_below_30pct_ff\",axis=1,inplace=True)\n","# data.drop(\"samples_below_25pct_ff\",axis=1,inplace=True)\n","# data.drop(\"samples_below_20pct_ff\",axis=1,inplace=True)\n","# data.drop(\"samples_below_15pct_ff\",axis=1,inplace=True)\n","# data.drop(\"samples_below_10pct_ff\",axis=1,inplace=True)\n","# data.drop(\"samples_below_5pct_ff\",axis=1,inplace=True)\n","\n","# here you can check the column has been droped\n","data.columns # this gives the column name which are persent in our data no Unnamed: 32 is not now there"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['Date', 'A', 'B', 'C', 'D', 'E', 'F', 'G'], dtype='object')"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"9qa2cYTopjY5","outputId":"620b31f3-8b52-40f8-d8b0-284c4265355d","executionInfo":{"status":"ok","timestamp":1660973869271,"user_tz":-300,"elapsed":447,"user":{"displayName":"Basharat Hussain","userId":"13397079541735527718"}},"colab":{"base_uri":"https://localhost:8080/","height":337}},"source":["\n","print(data.shape)\n","print(\"-----------------------------------\")\n","data.describe() # this will describe the all statistical function of our data"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["(11, 8)\n","-----------------------------------\n"]},{"output_type":"execute_result","data":{"text/plain":["               A           B           C          D          E          F  \\\n","count  11.000000   11.000000   11.000000  11.000000  11.000000  11.000000   \n","mean   24.227273  163.888182  115.155455  27.061818  19.665455  28.410000   \n","std     0.239879    0.647438    0.352686   0.795510   0.487737   0.759697   \n","min    23.810000  163.220000  114.730000  25.780000  19.010000  27.040000   \n","25%    24.085000  163.395000  114.845000  26.505000  19.370000  27.930000   \n","50%    24.190000  163.590000  115.110000  27.090000  19.610000  28.250000   \n","75%    24.360000  164.325000  115.470000  27.615000  19.965000  29.020000   \n","max    24.680000  164.930000  115.720000  28.220000  20.420000  29.510000   \n","\n","               G  \n","count  11.000000  \n","mean   59.119091  \n","std     1.964568  \n","min    56.180000  \n","25%    58.070000  \n","50%    59.410000  \n","75%    59.800000  \n","max    63.440000  "],"text/html":["\n","  <div id=\"df-904028db-7a17-4b45-8526-d5520a28685c\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>A</th>\n","      <th>B</th>\n","      <th>C</th>\n","      <th>D</th>\n","      <th>E</th>\n","      <th>F</th>\n","      <th>G</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>11.000000</td>\n","      <td>11.000000</td>\n","      <td>11.000000</td>\n","      <td>11.000000</td>\n","      <td>11.000000</td>\n","      <td>11.000000</td>\n","      <td>11.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>24.227273</td>\n","      <td>163.888182</td>\n","      <td>115.155455</td>\n","      <td>27.061818</td>\n","      <td>19.665455</td>\n","      <td>28.410000</td>\n","      <td>59.119091</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>0.239879</td>\n","      <td>0.647438</td>\n","      <td>0.352686</td>\n","      <td>0.795510</td>\n","      <td>0.487737</td>\n","      <td>0.759697</td>\n","      <td>1.964568</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>23.810000</td>\n","      <td>163.220000</td>\n","      <td>114.730000</td>\n","      <td>25.780000</td>\n","      <td>19.010000</td>\n","      <td>27.040000</td>\n","      <td>56.180000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>24.085000</td>\n","      <td>163.395000</td>\n","      <td>114.845000</td>\n","      <td>26.505000</td>\n","      <td>19.370000</td>\n","      <td>27.930000</td>\n","      <td>58.070000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>24.190000</td>\n","      <td>163.590000</td>\n","      <td>115.110000</td>\n","      <td>27.090000</td>\n","      <td>19.610000</td>\n","      <td>28.250000</td>\n","      <td>59.410000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>24.360000</td>\n","      <td>164.325000</td>\n","      <td>115.470000</td>\n","      <td>27.615000</td>\n","      <td>19.965000</td>\n","      <td>29.020000</td>\n","      <td>59.800000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>24.680000</td>\n","      <td>164.930000</td>\n","      <td>115.720000</td>\n","      <td>28.220000</td>\n","      <td>20.420000</td>\n","      <td>29.510000</td>\n","      <td>63.440000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-904028db-7a17-4b45-8526-d5520a28685c')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-904028db-7a17-4b45-8526-d5520a28685c button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-904028db-7a17-4b45-8526-d5520a28685c');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"Oex88qd38Thn"},"source":["You can build put inputs into a vector and then use pandas .cumsum() function to build the sequence for the time series:\n"]},{"cell_type":"code","metadata":{"id":"m_NRnXsJzphf","executionInfo":{"status":"ok","timestamp":1660973874031,"user_tz":-300,"elapsed":370,"user":{"displayName":"Basharat Hussain","userId":"13397079541735527718"}}},"source":["#input_cols = ['Date', 'A', 'B', 'C', 'D', 'E', 'F']\n","input_cols = ['A', 'B', 'C', 'D', 'E', 'F']\n","\n","# Put your inputs into a single list\n","data['single_input_vector'] = data[input_cols].apply(tuple, axis=1).apply(list)\n","# Double-encapsulate list so that you can sum it in the next step and keep time steps as separate elements\n","data['single_input_vector'] = data.single_input_vector.apply(lambda x: [list(x)])\n","# Use .cumsum() to include previous row vectors in the current row list of vectors\n","data['cumulative_input_vectors'] = data.single_input_vector.cumsum()"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sYepGo_UEEc-"},"source":["The output can be set up in a similar way, but it will be a single vector instead of a sequence:"]},{"cell_type":"code","metadata":{"id":"vo2EPal3Dz9z","executionInfo":{"status":"ok","timestamp":1660973882609,"user_tz":-300,"elapsed":365,"user":{"displayName":"Basharat Hussain","userId":"13397079541735527718"}}},"source":["output_cols = ['G']\n","# If your output is multi-dimensional, you need to capture those dimensions in one object\n","# If your output is a single dimension, this step may be unnecessary\n","data['output_vector'] = data[output_cols].apply(tuple, axis=1).apply(list)"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6BZolAh1EHXp"},"source":["The input sequences have to be the same length to run them through the model, so you need to pad them to be the max length of your cumulative vectors:"]},{"cell_type":"code","metadata":{"id":"3lFxiMRSEWqR","executionInfo":{"status":"ok","timestamp":1660973889587,"user_tz":-300,"elapsed":2680,"user":{"displayName":"Basharat Hussain","userId":"13397079541735527718"}}},"source":["# Pad your sequences so they are the same length\n","from keras.preprocessing.sequence import pad_sequences\n","from pandas import Series, DataFrame\n","\n","max_sequence_length = data.cumulative_input_vectors.apply(len).max()\n","# Save it as a list   \n","padded_sequences = pad_sequences(data.cumulative_input_vectors.tolist(), max_sequence_length).tolist()\n","data['padded_input_vectors'] = pd.Series(padded_sequences).apply(np.asarray)"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"79RWrzo-FsQk"},"source":["Training data can be pulled from the dataframe and put into numpy arrays. Note that the input data that comes out of the dataframe will not make a 3D array. It makes an array of arrays, which is not the same thing.\n","\n","You can use hstack and reshape to build a 3D input array."]},{"cell_type":"code","metadata":{"id":"KuTxIcAmFtcy","executionInfo":{"status":"ok","timestamp":1660973891299,"user_tz":-300,"elapsed":3,"user":{"displayName":"Basharat Hussain","userId":"13397079541735527718"}}},"source":["# Extract your training data\n","X_train_init = np.asarray(data.padded_input_vectors)\n","# Use hstack to and reshape to make the inputs a 3d vector\n","X_train = np.hstack(X_train_init).reshape(len(data),max_sequence_length,len(input_cols))\n","y_train = np.hstack(np.asarray(data.output_vector)).reshape(len(data),len(output_cols))"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nRLGg8FDGBl1"},"source":["Testing so far"]},{"cell_type":"code","metadata":{"id":"Ikb2IUjnGDsl","colab":{"base_uri":"https://localhost:8080/"},"outputId":"45144381-68a1-48a2-d606-454eb42e9533","executionInfo":{"status":"ok","timestamp":1660973894613,"user_tz":-300,"elapsed":3,"user":{"displayName":"Basharat Hussain","userId":"13397079541735527718"}}},"source":["print(X_train_init.shape)\n","print(X_train.shape)\n","print(X_train == X_train_init)"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["(11,)\n","(11, 11, 6)\n","False\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n","  This is separate from the ipykernel package so we can avoid doing imports until\n"]}]},{"cell_type":"markdown","metadata":{"id":"OJ_W7iSFGSzb"},"source":["Once you have training data you can define the dimensions of your input layer and output layers."]},{"cell_type":"code","metadata":{"id":"g5t-SivKGRU6","executionInfo":{"status":"ok","timestamp":1660973897311,"user_tz":-300,"elapsed":2,"user":{"displayName":"Basharat Hussain","userId":"13397079541735527718"}}},"source":["# Get your input dimensions\n","# Input length is the length for one input sequence (i.e. the number of rows for your sample)\n","# Input dim is the number of dimensions in one input vector (i.e. number of input columns)\n","input_length = X_train.shape[1]\n","input_dim = X_train.shape[2]\n","# Output dimensions is the shape of a single output vector\n","# In this case it's just 1, but it could be more\n","output_dim = len(y_train[0])"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mchq91TBGaS3"},"source":["Build the model:"]},{"cell_type":"code","metadata":{"id":"WSTAkiaBGavz","executionInfo":{"status":"ok","timestamp":1660973902921,"user_tz":-300,"elapsed":989,"user":{"displayName":"Basharat Hussain","userId":"13397079541735527718"}}},"source":["from keras.models import Model, Sequential\n","from keras.layers import LSTM, Dense\n","\n","# Build the model\n","model = Sequential()\n","\n","# I arbitrarily picked the output dimensions as 4\n","model.add(LSTM(4, input_dim = input_dim, input_length = input_length))\n","# The max output value is > 1 so relu is used as final activation.\n","model.add(Dense(output_dim, activation='relu'))\n","\n","model.compile(loss='mean_squared_error',\n","              optimizer='sgd',\n","              metrics=['accuracy'])"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"PuHs_UZ-GkJK","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"231e1097-25d5-45e7-8e7c-6562fba1c799","executionInfo":{"status":"ok","timestamp":1572547019161,"user_tz":-300,"elapsed":3080,"user":{"displayName":"Basharat Hussain","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDXtH5g0C7HMyOERBmIREHU1ZvtrNx98-DPcRb_=s64","userId":"13397079541735527718"}}},"source":["# Set batch_size to 7 to show that it doesn't have to be a factor or multiple of your sample size\n","history = model.fit(X_train, y_train,\n","              batch_size=7, nb_epoch=50,\n","              verbose = 1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/50\n","11/11 [==============================] - 0s 2ms/step - loss: 52.7813 - acc: 0.0000e+00\n","Epoch 2/50\n","11/11 [==============================] - 0s 2ms/step - loss: 52.2858 - acc: 0.0000e+00\n","Epoch 3/50\n","11/11 [==============================] - 0s 1ms/step - loss: 52.0275 - acc: 0.0000e+00\n","Epoch 4/50\n","11/11 [==============================] - 0s 2ms/step - loss: 51.7764 - acc: 0.0000e+00\n","Epoch 5/50\n","11/11 [==============================] - 0s 1ms/step - loss: 51.5528 - acc: 0.0000e+00\n","Epoch 6/50\n","11/11 [==============================] - 0s 1ms/step - loss: 51.4040 - acc: 0.0000e+00\n","Epoch 7/50\n","11/11 [==============================] - 0s 1ms/step - loss: 50.8425 - acc: 0.0000e+00\n","Epoch 8/50\n","11/11 [==============================] - 0s 1ms/step - loss: 50.6567 - acc: 0.0000e+00\n","Epoch 9/50\n","11/11 [==============================] - 0s 1ms/step - loss: 50.3904 - acc: 0.0000e+00\n","Epoch 10/50\n","11/11 [==============================] - 0s 2ms/step - loss: 50.1509 - acc: 0.0000e+00\n","Epoch 11/50\n","11/11 [==============================] - 0s 1ms/step - loss: 49.6791 - acc: 0.0000e+00\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n","  This is separate from the ipykernel package so we can avoid doing imports until\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 12/50\n","11/11 [==============================] - 0s 1ms/step - loss: 49.4448 - acc: 0.0000e+00\n","Epoch 13/50\n","11/11 [==============================] - 0s 1ms/step - loss: 49.2546 - acc: 0.0000e+00\n","Epoch 14/50\n","11/11 [==============================] - 0s 1ms/step - loss: 48.9598 - acc: 0.0000e+00\n","Epoch 15/50\n","11/11 [==============================] - 0s 1ms/step - loss: 48.7100 - acc: 0.0000e+00\n","Epoch 16/50\n","11/11 [==============================] - 0s 2ms/step - loss: 48.6208 - acc: 0.0000e+00\n","Epoch 17/50\n","11/11 [==============================] - 0s 1ms/step - loss: 48.3081 - acc: 0.0000e+00\n","Epoch 18/50\n","11/11 [==============================] - 0s 2ms/step - loss: 48.0894 - acc: 0.0000e+00\n","Epoch 19/50\n","11/11 [==============================] - 0s 1ms/step - loss: 47.9621 - acc: 0.0000e+00\n","Epoch 20/50\n","11/11 [==============================] - 0s 1ms/step - loss: 47.3647 - acc: 0.0000e+00\n","Epoch 21/50\n","11/11 [==============================] - 0s 1ms/step - loss: 47.1807 - acc: 0.0000e+00\n","Epoch 22/50\n","11/11 [==============================] - 0s 2ms/step - loss: 46.9230 - acc: 0.0000e+00\n","Epoch 23/50\n","11/11 [==============================] - 0s 2ms/step - loss: 46.8468 - acc: 0.0000e+00\n","Epoch 24/50\n","11/11 [==============================] - 0s 1ms/step - loss: 46.3064 - acc: 0.0000e+00\n","Epoch 25/50\n","11/11 [==============================] - 0s 1ms/step - loss: 45.9252 - acc: 0.0000e+00\n","Epoch 26/50\n","11/11 [==============================] - 0s 1ms/step - loss: 45.8259 - acc: 0.0000e+00\n","Epoch 27/50\n","11/11 [==============================] - 0s 1ms/step - loss: 45.4712 - acc: 0.0000e+00\n","Epoch 28/50\n","11/11 [==============================] - 0s 1ms/step - loss: 45.2914 - acc: 0.0000e+00\n","Epoch 29/50\n","11/11 [==============================] - 0s 1ms/step - loss: 45.0882 - acc: 0.0000e+00\n","Epoch 30/50\n","11/11 [==============================] - 0s 1ms/step - loss: 44.7549 - acc: 0.0000e+00\n","Epoch 31/50\n","11/11 [==============================] - 0s 1ms/step - loss: 44.4514 - acc: 0.0000e+00\n","Epoch 32/50\n","11/11 [==============================] - 0s 1ms/step - loss: 44.1061 - acc: 0.0000e+00\n","Epoch 33/50\n","11/11 [==============================] - 0s 1ms/step - loss: 43.9084 - acc: 0.0000e+00\n","Epoch 34/50\n","11/11 [==============================] - 0s 1ms/step - loss: 43.5432 - acc: 0.0000e+00\n","Epoch 35/50\n","11/11 [==============================] - 0s 2ms/step - loss: 43.3407 - acc: 0.0000e+00\n","Epoch 36/50\n","11/11 [==============================] - 0s 2ms/step - loss: 43.0994 - acc: 0.0000e+00\n","Epoch 37/50\n","11/11 [==============================] - 0s 1ms/step - loss: 42.8715 - acc: 0.0000e+00\n","Epoch 38/50\n","11/11 [==============================] - 0s 1ms/step - loss: 42.5002 - acc: 0.0000e+00\n","Epoch 39/50\n","11/11 [==============================] - 0s 1ms/step - loss: 42.3303 - acc: 0.0000e+00\n","Epoch 40/50\n","11/11 [==============================] - 0s 1ms/step - loss: 41.9603 - acc: 0.0000e+00\n","Epoch 41/50\n","11/11 [==============================] - 0s 2ms/step - loss: 41.7444 - acc: 0.0000e+00\n","Epoch 42/50\n","11/11 [==============================] - 0s 2ms/step - loss: 41.6266 - acc: 0.0000e+00\n","Epoch 43/50\n","11/11 [==============================] - 0s 2ms/step - loss: 41.4344 - acc: 0.0000e+00\n","Epoch 44/50\n","11/11 [==============================] - 0s 2ms/step - loss: 41.1669 - acc: 0.0000e+00\n","Epoch 45/50\n","11/11 [==============================] - 0s 2ms/step - loss: 40.9583 - acc: 0.0000e+00\n","Epoch 46/50\n","11/11 [==============================] - 0s 2ms/step - loss: 40.8224 - acc: 0.0000e+00\n","Epoch 47/50\n","11/11 [==============================] - 0s 2ms/step - loss: 40.8296 - acc: 0.0000e+00\n","Epoch 48/50\n","11/11 [==============================] - 0s 2ms/step - loss: 40.2078 - acc: 0.0000e+00\n","Epoch 49/50\n","11/11 [==============================] - 0s 2ms/step - loss: 40.0593 - acc: 0.0000e+00\n","Epoch 50/50\n","11/11 [==============================] - 0s 2ms/step - loss: 39.9286 - acc: 0.0000e+00\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tgEdVgFYHPjX","executionInfo":{"status":"ok","timestamp":1660973919413,"user_tz":-300,"elapsed":570,"user":{"displayName":"Basharat Hussain","userId":"13397079541735527718"}}},"source":[""],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LqwvpemGHBuc"},"source":["Use model.predict(X) where X is the same format (other than the number of samples) as X_train in order to make predictions from the model."]},{"cell_type":"code","metadata":{"id":"QO9bAI5VHQdK","executionInfo":{"status":"ok","timestamp":1660973922289,"user_tz":-300,"elapsed":423,"user":{"displayName":"Basharat Hussain","userId":"13397079541735527718"}}},"source":["#prediction code in here"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"BC-eVnVDot4K","outputId":"fd9bf850-3d7d-4565-b2a2-da14e9ad69b1","executionInfo":{"status":"error","timestamp":1660973924121,"user_tz":-300,"elapsed":480,"user":{"displayName":"Basharat Hussain","userId":"13397079541735527718"}},"colab":{"base_uri":"https://localhost:8080/","height":244}},"source":["from datetime import datetime\n","\n","graph_xy_var = 'utc_time_id'\n","# now these are the variables which will use for prediction\n","X_graph_data = np.array(train[graph_xy_var])     # taking the training data input \n","\n","arr = []\n","list = X_graph_data;\n","index=0 #Start at the first element\n","while(index < len(list)):   # Until we get to the end of the list\n","    KK = (pd.to_datetime(list[index]) - pd.to_datetime(list[0])).total_seconds()\n","    # KK = (pd.to_datetime(list[0]) - pd.to_datetime(list[index])).dt.total_seconds()\n","    arr.append( KK );\n","    index+=1 #Move The first element n elements forward\n","\n","print(X_graph_data);\n","print(arr)\n"," \n","Y_graph_data = np.array(train['avg_freeflow_speed'], )\n","print(Y_graph_data);\n"," \n","import altair as alt\n","alt.Chart(train).mark_line().encode(\n","  x='utc_time_id:T',\n","  y='avg_travel_time',\n","  color='avg_freeflow_speed'\n",").interactive(bind_y=False)"],"execution_count":14,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-57ba12ec85b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mgraph_xy_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utc_time_id'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# now these are the variables which will use for prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mX_graph_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgraph_xy_var\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# taking the training data input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"]}]},{"cell_type":"code","metadata":{"id":"KxcjV_1S1aKG","executionInfo":{"status":"ok","timestamp":1660973933910,"user_tz":-300,"elapsed":380,"user":{"displayName":"Basharat Hussain","userId":"13397079541735527718"}}},"source":["# Graph try\n","prediction_var = [\n","        'utc_time_id', \n","        'avg_freeflow_speed', \n","        'avg_travel_time',\n","       'high_quality_samples', 'samples_below_100pct_ff', 'samples_below_95pct_ff', 'samples_below_90pct_ff',\n","       'samples_below_85pct_ff', 'samples_below_80pct_ff', 'samples_below_75pct_ff', 'samples_below_70pct_ff',\n","       'samples_below_65pct_ff', 'samples_below_60pct_ff', 'samples_below_55pct_ff', 'samples_below_50pct_ff',\n","       'samples_below_45pct_ff', 'samples_below_40pct_ff', 'samples_below_35pct_ff', 'samples_below_30pct_ff',\n","       'samples_below_25pct_ff', 'samples_below_20pct_ff', 'samples_below_15pct_ff', 'samples_below_10pct_ff',\n","       'samples_below_5pct_ff']"],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bL4z0BArzqff"},"source":["Now I'm going to use Keras to define a Neural network that will be trained off of this data. This Neural Network can then be used to predict future values for breast cancer presence. "]},{"cell_type":"code","metadata":{"id":"LoWtmalR80De","executionInfo":{"status":"ok","timestamp":1660973940306,"user_tz":-300,"elapsed":382,"user":{"displayName":"Basharat Hussain","userId":"13397079541735527718"}}},"source":["from keras.models import Sequential\n","from keras.layers import Dense\n","\n","classifier = Sequential() # Initialising the ANN\n","\n","classifier.add(Dense(units = 100, activation = 'relu', input_dim = 25))\n","classifier.add(Dense(units = 50, activation = 'tanh'))\n","classifier.add(Dense(units = 20, activation = 'relu'))\n","classifier.add(Dense(units = 1, activation = 'sigmoid'))"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cJ_MStxe9CbN"},"source":["Once the Neural Network is defined, I now have to specify is the optimizer and loss function"]},{"cell_type":"code","metadata":{"id":"wR2O00E39C7E","executionInfo":{"status":"ok","timestamp":1660973943143,"user_tz":-300,"elapsed":398,"user":{"displayName":"Basharat Hussain","userId":"13397079541735527718"}}},"source":["classifier.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy')"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mn9WPqaP9RmN"},"source":["You now train the neural network using Classifier.fit, passing it the training data -- i.e. for this set of X, this is what the Y should look like. The NN will then spot the patterns in the data, and build a neural network that could replicate that. "]},{"cell_type":"code","metadata":{"id":"1QlYnFgH9N49","outputId":"7aad741d-d3e5-4b99-80bc-af59f6741e96","executionInfo":{"status":"error","timestamp":1660973945100,"user_tz":-300,"elapsed":6,"user":{"displayName":"Basharat Hussain","userId":"13397079541735527718"}},"colab":{"base_uri":"https://localhost:8080/","height":171}},"source":["classifier.fit(X_train, Y_train, batch_size = 1, epochs = 10)"],"execution_count":18,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-0eaeab3030bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'Y_train' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"yrnGgtfn9gsE"},"source":["To predict new values, the Neural Network uses classifier.predict. I'm going to pass it the test values for X (which the Neural Network hasn't previously seen) and it will give me back a set of predictions. These predicitons will be probabilities, so I will clean them up by saying that if thye are greater than .5, I'll make them 1, else I'll make them 0."]},{"cell_type":"code","metadata":{"id":"xEaG0Tkx9fUU","colab":{"base_uri":"https://localhost:8080/","height":244},"executionInfo":{"status":"error","timestamp":1660973950440,"user_tz":-300,"elapsed":368,"user":{"displayName":"Basharat Hussain","userId":"13397079541735527718"}},"outputId":"501cd2a9-b985-44e9-a776-5f904b127750"},"source":["Y_pred = classifier.predict(X_test)\n","# print(Y_pred);\n","Y_pred = [ 1 if y>=0.5 else 0 for y in Y_pred ]\n","\n","print(Y_pred);"],"execution_count":19,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-e36a3d1a3135>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mY_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# print(Y_pred);\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mY_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0;36m0.5\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mY_pred\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'X_test' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"oQK3Qo9h97OW"},"source":["Now we can loop through the set of predicitons for the test set (called Y_pred) and the actual values for the test set (celled Y_test), and see how alike they are -- if they are the same, I'll increment 'correct', otherwise I'll incrememnt 'wrong'. \n","\n","You'll see the result is 100% accurate, even though the neural network reported a lower accuracy than that. Why? "]},{"cell_type":"code","metadata":{"id":"he_2VHJ_9yC1","outputId":"57b5140d-443f-46ff-e0e6-69f60686cfbd","executionInfo":{"status":"error","timestamp":1660973956893,"user_tz":-300,"elapsed":394,"user":{"displayName":"Basharat Hussain","userId":"13397079541735527718"}},"colab":{"base_uri":"https://localhost:8080/","height":244}},"source":["total = 0\n","correct = 0\n","wrong = 0\n","print(Y_pred[9]);\n","for i in Y_pred:\n","  total=total+1\n","  \n","  if(Y_test.at[i,0] == Y_pred[i]):\n","    correct=correct+1\n","  else:\n","    wrong=wrong+1\n","\n","print(\"Total \" + str(total))\n","print(\"Correct \" + str(correct))\n","print(\"Wrong \" + str(wrong))"],"execution_count":20,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-147de1bbad14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mwrong\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mY_pred\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'Y_pred' is not defined"]}]}]}